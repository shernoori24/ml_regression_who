{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0c8e2b",
   "metadata": {},
   "source": [
    "# Gestion des imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4879b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfe047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12415086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223a492",
   "metadata": {},
   "source": [
    "# Chargement du CSV et vérification des infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f53467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informations du dataframe covid:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2730307 entries, 0 to 2730306\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   observation_date  object \n",
      " 1   country           object \n",
      " 2   province_state    object \n",
      " 3   city              object \n",
      " 4   total_cases       int64  \n",
      " 5   new_cases         float64\n",
      " 6   total_deaths      float64\n",
      " 7   new_deaths        float64\n",
      " 8   total_recovered   float64\n",
      " 9   active_cases      float64\n",
      "dtypes: float64(5), int64(1), object(4)\n",
      "memory usage: 208.3+ MB\n",
      "None\n",
      "\n",
      "Informations du dataframe locations:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4163 entries, 0 to 4162\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   iso3            4163 non-null   object \n",
      " 1   city            3223 non-null   object \n",
      " 2   province_state  3966 non-null   object \n",
      " 3   country_region  4163 non-null   object \n",
      " 4   latitude        4163 non-null   float64\n",
      " 5   longitude       4163 non-null   float64\n",
      " 6   combined_key    4163 non-null   object \n",
      " 7   population      4163 non-null   int64  \n",
      " 8   who_region      4163 non-null   object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 292.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Charger les fichiers des données covid et localisations\n",
    "df = pd.read_csv('full_clean_covid_19.csv')\n",
    "locations = pd.read_csv('full_clean_locations.csv')\n",
    "\n",
    "# Afficher les informations des dataframes\n",
    "print(\"Informations du dataframe covid:\")\n",
    "print(df.info())\n",
    "print(\"\\nInformations du dataframe locations:\")\n",
    "print(locations.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5428262",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'country_region'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_21320\\54416767.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Intégrer les colonnes 'population' et 'who_region' dans le dataframe principal\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Merger les deux dataframes sur les colonnes géographiques communes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = df.merge(locations[['country_region', 'province_state', 'city', 'population', 'who_region']], \n\u001b[32m      4\u001b[39m               on=[\u001b[33m'country_region'\u001b[39m, \u001b[33m'province_state'\u001b[39m, \u001b[33m'city'\u001b[39m],\n\u001b[32m      5\u001b[39m               how=\u001b[33m'left'\u001b[39m)\n\u001b[32m      6\u001b[39m \n",
      "\u001b[32mc:\\Users\\stagiaire\\Documents\\UIMM IA\\Machine Learning Joyce\\exo_covid\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10855\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10856\u001b[39m     ) -> DataFrame:\n\u001b[32m  10857\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10858\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10859\u001b[39m         return merge(\n\u001b[32m  10860\u001b[39m             self,\n\u001b[32m  10861\u001b[39m             right,\n\u001b[32m  10862\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\stagiaire\\Documents\\UIMM IA\\Machine Learning Joyce\\exo_covid\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\stagiaire\\Documents\\UIMM IA\\Machine Learning Joyce\\exo_covid\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\stagiaire\\Documents\\UIMM IA\\Machine Learning Joyce\\exo_covid\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\stagiaire\\Documents\\UIMM IA\\Machine Learning Joyce\\exo_covid\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'country_region'"
     ]
    }
   ],
   "source": [
    "# Intégrer les colonnes 'population' et 'who_region' dans le dataframe principal\n",
    "# Merger les deux dataframes sur les colonnes géographiques communes\n",
    "df = df.merge(locations[['country_region', 'province_state', 'city', 'population', 'who_region']], \n",
    "              on=['country_region', 'province_state', 'city'], \n",
    "              how='left')\n",
    "\n",
    "# Supprimer les colonnes 'country', 'province_state' et 'city'\n",
    "if 'country' in df.columns:\n",
    "    df = df.drop(columns=['country'])\n",
    "df = df.drop(columns=['country_region', 'province_state', 'city'])\n",
    "\n",
    "# Vérifier les informations du nouveau dataframe\n",
    "print(\"Informations après intégration:\")\n",
    "print(df.info())\n",
    "print(f\"Forme du dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ae8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les enregistrements avec des valeurs nulles dans les colonnes 'population' et 'who_region'\n",
    "print(\"Valeurs nulles avant nettoyage:\")\n",
    "print(df[['population', 'who_region']].isnull().sum())\n",
    "\n",
    "df = df.dropna(subset=['population', 'who_region'])\n",
    "\n",
    "print(f\"Forme après suppression des NaN: {df.shape}\")\n",
    "print(\"Valeurs nulles après nettoyage:\")\n",
    "print(df[['population', 'who_region']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bc3fe",
   "metadata": {},
   "source": [
    "# Définition d'un lot aléatoire de 5 000 lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garder 5000 lignes aléatoires\n",
    "df_simple = df.sample(n=5000, random_state=42)\n",
    "print(f\"Nouveau dataframe: {df_simple.shape}\")\n",
    "print(df_simple.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2774ac",
   "metadata": {},
   "source": [
    "# Ajout du taux de mortalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le taux de mortalité dans une nouvelle colonne : \"mortality_rate\"\n",
    "df_simple['mortality_rate'] = df_simple['total_deaths'] / df_simple['total_cases']\n",
    "\n",
    "# Afficher le résultat en pourcentage\n",
    "df_simple['mortality_rate'] = df_simple['mortality_rate'] * 100\n",
    "\n",
    "print(\"Taux de mortalité calculé:\")\n",
    "print(f\"Min: {df_simple['mortality_rate'].min():.4f}%\")\n",
    "print(f\"Max: {df_simple['mortality_rate'].max():.4f}%\")\n",
    "print(f\"Moyenne: {df_simple['mortality_rate'].mean():.4f}%\")\n",
    "\n",
    "# Retirer les deux colonnes 'total_deaths' et 'total_cases'\n",
    "df_simple = df_simple.drop(columns=['total_deaths', 'total_cases'])\n",
    "print(f\"\\nNouvelles dimensions: {df_simple.shape}\")\n",
    "print(df_simple.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les enregistrements où 'mortality_rate' = infinity\n",
    "infinity_mask = np.isinf(df_simple['mortality_rate'])\n",
    "print(f\"Nombre d'enregistrements avec mortality_rate = infinity: {infinity_mask.sum()}\")\n",
    "\n",
    "if infinity_mask.sum() > 0:\n",
    "    print(\"Exemples d'enregistrements avec infinity:\")\n",
    "    print(df_simple[infinity_mask].head())\n",
    "\n",
    "# Supprimer les enregistrements où 'mortality_rate' = infinity\n",
    "df_simple = df_simple[~infinity_mask]\n",
    "print(f\"Nouvelles dimensions après suppression: {df_simple.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6529d61",
   "metadata": {},
   "source": [
    "# Ajout des jours, mois, années et saisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer la colonne 'observation_date' en plusieurs colonnes : 'month' (string) et 'season' (string)\n",
    "\n",
    "# On (re)convertit en datetime par sécurité\n",
    "df_simple['observation_date'] = pd.to_datetime(df_simple['observation_date'])\n",
    "\n",
    "# On crée une nouvelle colonne 'day' dans laquelle on extrait le jour de la date\n",
    "df_simple['day'] = df_simple['observation_date'].dt.day\n",
    "\n",
    "# Dictionnaire des mois en français\n",
    "mois_francais = {\n",
    "    1: 'Janvier', 2: 'Février', 3: 'Mars', 4: 'Avril',\n",
    "    5: 'Mai', 6: 'Juin', 7: 'Juillet', 8: 'Août',\n",
    "    9: 'Septembre', 10: 'Octobre', 11: 'Novembre', 12: 'Décembre'\n",
    "}\n",
    "\n",
    "# On crée une nouvelle colonne 'month' dans laquelle on extrait le mois de la date en string\n",
    "df_simple['month'] = df_simple['observation_date'].dt.month.map(mois_francais)\n",
    "\n",
    "# On créer une nouvelle colonne 'year' dans laquelle on extrait l'année de la date\n",
    "df_simple['year'] = df_simple['observation_date'].dt.year\n",
    "\n",
    "# On crée une nouvelle colonne 'day_of_week' dans laquelle on extrait le jour de la semaine (0 = lundi, 6 = dimanche)\n",
    "df_simple['day_of_week'] = df_simple['observation_date'].dt.dayofweek\n",
    "\n",
    "# Fonction pour déterminer la saison\n",
    "def obtenir_saison(date):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    \n",
    "    if (month == 3 and day >= 21) or month in [4, 5] or (month == 6 and day <= 20):\n",
    "        return 'spring'\n",
    "    elif (month == 6 and day >= 21) or month in [7, 8] or (month == 9 and day <= 20):\n",
    "        return 'summer'\n",
    "    elif (month == 9 and day >= 21) or month in [10, 11] or (month == 12 and day <= 20):\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'\n",
    "\n",
    "# On crée une nouvelle colonne 'season' en fonction de la date d'observation\n",
    "df_simple['season'] = df_simple['observation_date'].apply(obtenir_saison)\n",
    "\n",
    "# On supprime la colonne 'observation_date'\n",
    "df_simple = df_simple.drop(columns=['observation_date'])\n",
    "\n",
    "print(\"Nouvelles colonnes temporelles créées:\")\n",
    "print(f\"Colonnes: {df_simple.columns.tolist()}\")\n",
    "print(f\"Dimensions: {df_simple.shape}\")\n",
    "print(df_simple.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689412b5",
   "metadata": {},
   "source": [
    "# Gestion des valeurs quantitatives manquantes (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09315bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les valeurs Nan dans les colonnes quantitatives\n",
    "print(\"Valeurs NaN par colonne:\")\n",
    "print(df_simple.isnull().sum())\n",
    "\n",
    "# Identifier les colonnes quantitatives\n",
    "colonnes_numeriques = df_simple.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nColonnes numériques: {colonnes_numeriques}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f43522",
   "metadata": {},
   "source": [
    "Imputer les valeurs quantitatives NaN par la moyenne ou la médiane de la colonne : 'total_recovered', 'active_cases'\n",
    "Justifier ce choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde la distribution de la colonne 'total_recovered' en calculant la moyenne, la médiane, les écarts-types et les quantiles Q1 Q2 Q3\n",
    "if 'total_recovered' in df_simple.columns:\n",
    "    print(\"Distribution de 'total_recovered':\")\n",
    "    print(f\"Moyenne: {df_simple['total_recovered'].mean():.2f}\")\n",
    "    print(f\"Médiane: {df_simple['total_recovered'].median():.2f}\")\n",
    "    print(f\"Écart-type: {df_simple['total_recovered'].std():.2f}\")\n",
    "    print(f\"Q1: {df_simple['total_recovered'].quantile(0.25):.2f}\")\n",
    "    print(f\"Q2 (médiane): {df_simple['total_recovered'].quantile(0.5):.2f}\")\n",
    "    print(f\"Q3: {df_simple['total_recovered'].quantile(0.75):.2f}\")\n",
    "    \n",
    "    # Imputer par la médiane (plus robuste aux outliers)\n",
    "    df_simple['total_recovered'] = df_simple['total_recovered'].fillna(df_simple['total_recovered'].median())\n",
    "    print(f\"\\nValeurs NaN de 'total_recovered' imputées avec la médiane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699054f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde la distribution de la colonne 'active_cases' en calculant la moyenne, la médiane, les écarts-types et les quantiles Q1 Q2 Q3\n",
    "if 'active_cases' in df_simple.columns:\n",
    "    print(\"Distribution de 'active_cases':\")\n",
    "    print(f\"Moyenne: {df_simple['active_cases'].mean():.2f}\")\n",
    "    print(f\"Médiane: {df_simple['active_cases'].median():.2f}\")\n",
    "    print(f\"Écart-type: {df_simple['active_cases'].std():.2f}\")\n",
    "    print(f\"Q1: {df_simple['active_cases'].quantile(0.25):.2f}\")\n",
    "    print(f\"Q2 (médiane): {df_simple['active_cases'].quantile(0.5):.2f}\")\n",
    "    print(f\"Q3: {df_simple['active_cases'].quantile(0.75):.2f}\")\n",
    "    \n",
    "    # Imputer par la médiane (plus robuste aux outliers)  \n",
    "    df_simple['active_cases'] = df_simple['active_cases'].fillna(df_simple['active_cases'].median())\n",
    "    print(f\"\\nValeurs NaN de 'active_cases' imputées avec la médiane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On vérifie qu'il n'y a plus de NaN\n",
    "print(\"Vérification - Valeurs NaN restantes:\")\n",
    "print(df_simple.isnull().sum())\n",
    "print(f\"\\nTotal des NaN dans le dataframe: {df_simple.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a00b3",
   "metadata": {},
   "source": [
    "# Transformation des valeurs strings en booléens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de pandas get_dummies pour transformer toutes les valeurs strings en booléens\n",
    "print(\"Colonnes avant transformation:\")\n",
    "print(df_simple.dtypes)\n",
    "\n",
    "# Transformer les variables catégorielles en variables dummy\n",
    "df_simple = pd.get_dummies(df_simple, drop_first=True)\n",
    "\n",
    "print(f\"\\nDimensions après transformation: {df_simple.shape}\")\n",
    "print(\"\\nColonnes après transformation:\")\n",
    "print(df_simple.columns.tolist())\n",
    "\n",
    "# Convertir les colonnes booléennes en entiers (0 et 1)\n",
    "colonnes_bool = df_simple.select_dtypes(include=['bool']).columns\n",
    "if len(colonnes_bool) > 0:\n",
    "    df_simple[colonnes_bool] = df_simple[colonnes_bool].astype(int)\n",
    "    print(f\"\\n{len(colonnes_bool)} colonnes booléennes converties en entiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc12c2",
   "metadata": {},
   "source": [
    "# Standardisation des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e04044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de StandardScaler pour standardiser les valeurs quantitatives non booléennes SAUF 'mortality_rate'\n",
    "\n",
    "# Identifier toutes les colonnes numériques\n",
    "colonnes_numeriques = df_simple.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Retirer la variable cible 'mortality_rate' de la liste\n",
    "colonnes_a_standardiser = [col for col in colonnes_numeriques if col != 'mortality_rate']\n",
    "\n",
    "print(f\"Colonnes à standardiser: {len(colonnes_a_standardiser)}\")\n",
    "\n",
    "# Vérifier et nettoyer les valeurs infinies\n",
    "df_simple[colonnes_a_standardiser] = df_simple[colonnes_a_standardiser].replace([np.inf, -np.inf], np.nan)\n",
    "df_simple[colonnes_a_standardiser] = df_simple[colonnes_a_standardiser].fillna(df_simple[colonnes_a_standardiser].median())\n",
    "\n",
    "# Appliquer la standardisation\n",
    "scaler = StandardScaler()\n",
    "df_simple[colonnes_a_standardiser] = scaler.fit_transform(df_simple[colonnes_a_standardiser])\n",
    "\n",
    "print(\"Standardisation appliquée avec succès!\")\n",
    "print(f\"Dimensions finales: {df_simple.shape}\")\n",
    "\n",
    "# Supprimer les colonnes constantes (qui n'apportent pas d'information)\n",
    "df_simple = df_simple.loc[:, df_simple.nunique() > 1]\n",
    "print(f\"Dimensions après suppression des colonnes constantes: {df_simple.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d60be",
   "metadata": {},
   "source": [
    "# Modèle de régression linéaire multiple \n",
    "- Séparer le jeu de données en train et test (70% train, 30% test) /!\\ mettre shuffle=true (on mélange les données avant de les séparer pour éviter un biais temporel)\n",
    "- Lancer le modèle \n",
    "- On fait un ajustement avec .fit()\n",
    "- Ressortir un tableau qui reprend pour chaque variable explicative ses différents coefficients (beta1, 2, 3...) -> savoir ce qui joue le plus grand rôle dans la variation du taux de mortalité\n",
    "- Calculer et afficher le R2 du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7caec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer le jeu de données en variables explicatives (X) et variable cible (y)\n",
    "Y = df_simple['mortality_rate']  # Variable cible : taux de mortalité\n",
    "X = df_simple.drop(columns=['mortality_rate'])  # Variables explicatives\n",
    "\n",
    "print(f\"Variables explicatives (X): {X.shape}\")\n",
    "print(f\"Variable cible (Y): {Y.shape}\")\n",
    "\n",
    "# Train test split avec shuffle=true (70% train, 30% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, Y_test: {Y_test.shape}\")\n",
    "\n",
    "# Initialiser le Modèle de régression linéaire multiple avec .fit() pour l'ajustement \n",
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"Modèle ajusté avec succès!\")\n",
    "\n",
    "# On fait des prédictions sur le jeu de test\n",
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Prédictions effectuées: {len(Y_pred)} valeurs\")\n",
    "\n",
    "# Ressortir un tableau qui reprend pour chaque variable explicative ses différents coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Coefficient (β)': model.coef_\n",
    "}).sort_values(by='Coefficient (β)', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 des variables les plus influentes:\")\n",
    "print(coefficients.head(15))\n",
    "\n",
    "# Calculer et afficher le R2 du modèle\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f\"\\nScore R² du modèle: {r2:.4f}\")\n",
    "print(f\"Le modèle explique {r2*100:.2f}% de la variance du taux de mortalité\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fa756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher graphiquement les 10 coefficients les plus importants\n",
    "top_10_coefficients = coefficients.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_10_coefficients)), top_10_coefficients['Coefficient (β)'])\n",
    "plt.yticks(range(len(top_10_coefficients)), top_10_coefficients['Variable'])\n",
    "plt.xlabel('Coefficient (β)')\n",
    "plt.title('Top 10 des Variables les Plus Influentes sur le Taux de Mortalité')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, v in enumerate(top_10_coefficients['Coefficient (β)']):\n",
    "    plt.text(v + (0.01 if v >= 0 else -0.01), i, f'{v:.3f}', \n",
    "             ha='left' if v >= 0 else 'right', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graphique de comparaison prédictions vs valeurs réelles\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(Y_test, Y_pred, alpha=0.6)\n",
    "plt.xlabel(\"Valeurs réelles du taux de mortalité (%)\")\n",
    "plt.ylabel(\"Prédictions du modèle (%)\")\n",
    "plt.title(f\"Prédictions vs Valeurs Réelles (R² = {r2:.4f})\")\n",
    "plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'r--', lw=2, label='Ligne parfaite')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
